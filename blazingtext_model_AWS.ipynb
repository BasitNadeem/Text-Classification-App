{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906f04b8-0b08-429f-9396-bfca830b5aaf",
   "metadata": {},
   "source": [
    "# Text Classification using SageMaker BlazingText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb92a70-0a6c-46b4-a9f5-ed5e56b9fe20",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2626d8-6e78-4da7-b391-6a8b8917120a",
   "metadata": {},
   "source": [
    "Specify S3 Bucket and prefix used for training and model data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6a3d5a-a1a9-4440-b15d-b2ddb2162823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::025730522839:role/service-role/AmazonSageMaker-ExecutionRole-20220817T161703\n",
      "sagemaker-us-east-1-025730522839\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()       # IAM role ARN \n",
    "print(role)  \n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "bucket = sess.default_bucket()    # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'fModelData'             # Replace with the prefix under which you want to store the data if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8407c4-0bb3-45c8-855e-1d183800bc8f",
   "metadata": {},
   "source": [
    "## Data Preperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37aceffc-7b96-4c92-91a0-f9c92b02eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, re, os\n",
    "\n",
    "# First, we need to upload train and test data files to S3 bucket and prefix location \n",
    "train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train_data.txt')           # Training data path from s3 bucket \n",
    "test_data = 's3://{}/{}/{}'.format(bucket, prefix, 'test_data_solution.txt')    # Test data path from s3 bucket \n",
    "\n",
    "#Train Data Prep. ( We will be using only Title & Genre for our model ) \n",
    "df_train = pd.read_csv(train_data, sep=\" :::\", header=None, names=['Id', 'Title', 'Genre', 'Desc.'], engine=\"python\")\n",
    "df_train.drop(['Id', 'Desc.'], axis=1, inplace=True)\n",
    "\n",
    "#Test Data Prep.\n",
    "df_test = pd.read_csv(test_data, sep=\" :::\", header=None, names=['Id', 'Title', 'Genre', 'Desc.'], engine=\"python\")\n",
    "df_test.drop(['Id', 'Desc.'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af72fb-b0cb-47b7-bf4e-c1b80e0af92b",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031e730-44ff-45a0-8407-fec4c34cc1c6",
   "metadata": {},
   "source": [
    "We need to preprocess the training data into space separated tokenized text format which can be consumed by **BlazingText** algorithm. Also, the class label(s) should be prefixed with __ __label__ __ and it should be present in the same line along with the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f0a487-7547-439c-bbf7-d3d0dc3b24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Data Preprocessing\n",
    "#Extract characters and numbers from strings and return with a prefix __label__ in the same line \n",
    "def Preprocess_Data(train, test):\n",
    "        def process_title(title):\n",
    "                \"\"\"\n",
    "                function that extracts characters and numbers\n",
    "                from strings.\n",
    "\n",
    "                input:\n",
    "                        title: string value\n",
    "\n",
    "                output:\n",
    "                        title: cleaned string value\n",
    "                \"\"\"\n",
    "                punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_“.|\"'''\n",
    "\n",
    "                for ele in punc:\n",
    "                        if ele in title:\n",
    "                                title = title.replace(ele, \"\")\n",
    "                # strip away numbers and parenthesis\n",
    "                title = (\n",
    "                        title.replace(\"(\", \"\")\n",
    "                        .replace(\")\", \"\")\n",
    "                        .replace(\"/\", \"\")\n",
    "                        .replace(\"_\", \"\")\n",
    "                        .replace(\"-\", \"\")\n",
    "                        .replace(\"&\", \"\")\n",
    "                        .replace(\":\", \"\")\n",
    "                        .replace(\"@\", \"\")\n",
    "                )\n",
    "                title = re.sub(r\"\\d+\", \"\", title)\n",
    "                title = title.replace(\"?\", \"\")\n",
    "                # strip away \"part\" word\n",
    "                title = re.sub(r\"[Pp]art\", \"\", title)\n",
    "                # strip II and III and IV\n",
    "                title = title.replace(\"II\", \"\").replace(\"III\", \"\").replace(\"IV\", \"\")\n",
    "                title = title.strip()\n",
    "                title = re.sub(\" +\", \" \", title)\n",
    "\n",
    "                return title\n",
    "\n",
    "        train['Title'] = train.Title.apply(process_title)\n",
    "        test['Title'] = test.Title.apply(process_title)\n",
    "\n",
    "        train[\"Genre\"] = train[\"Genre\"].apply(process_title)\n",
    "        test[\"Genre\"] = test[\"Genre\"].apply(process_title)\n",
    "        \n",
    "        train.iloc[:, 1] = train.iloc[:, 1].apply(lambda x: '__label__' + x)\n",
    "        test.iloc[:, 1] = test.iloc[:, 1].apply(lambda x: '__label__' + x)\n",
    "\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e000b658-1322-4c07-8da5-312c61b5f9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                   Title                 Genre\n",
       " 0                  Oscar et la dame rose        __label__drama\n",
       " 1                                  Cupid     __label__thriller\n",
       " 2               Young Wild and Wonderful        __label__adult\n",
       " 3                         The Secret Sin        __label__drama\n",
       " 4                        The Unrecovered        __label__drama\n",
       " ...                                  ...                   ...\n",
       " 54209                             Bonino       __label__comedy\n",
       " 54210                Dead Girls Dont Cry       __label__horror\n",
       " 54211  Ronald Goedemondt Ze bestaan echt  __label__documentary\n",
       " 54212                  Make Your Own Bed       __label__comedy\n",
       " 54213  Natures Fury Storm of the Century      __label__history\n",
       " \n",
       " [54214 rows x 2 columns],\n",
       "                       Title                 Genre\n",
       " 0              Edgars Lunch     __label__thriller\n",
       " 1         La guerra de papá       __label__comedy\n",
       " 2      Off the Beaten Track  __label__documentary\n",
       " 3           Meu Amigo Hindu        __label__drama\n",
       " 4                Er nu zhai        __label__drama\n",
       " ...                     ...                   ...\n",
       " 54195   Tales of Light Dark       __label__horror\n",
       " 54196  Der letzte Mohikaner      __label__western\n",
       " 54197          Oliver Twink        __label__adult\n",
       " 54198            Slipstream        __label__drama\n",
       " 54199    Curitiba Zero Grau        __label__drama\n",
       " \n",
       " [54200 rows x 2 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocess_Data(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552537e6-8af9-4c89-9090-835528ce1170",
   "metadata": {},
   "source": [
    "#### The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 bucket so that it can be consumed by SageMaker to execute training jobs. We’ll use Python SDK to upload these two files to the bucket and prefix location that we have set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56fe022-9c08-40ea-9977-eb8fe57ea2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + \"/train_processed\"        #Train channel for s3 bucket path\n",
    "test_channel = prefix + \"/test_processed\"          #Test channel for s3 bucket path\n",
    "\n",
    "# Save the Processed data as TXT files to upload:\n",
    "df_train[['Genre', 'Title']].to_csv('./train_processed.txt', index = False, sep = ' ', header = None,  escapechar = \" \")\n",
    "df_test[['Genre', 'Title']].to_csv('./test_processed.txt', index = False, sep = ' ', header = None,  escapechar = \" \")\n",
    "\n",
    "# Python SDK to upload these two files to the bucket and prefix location that we have set above.\n",
    "train_data_uri = sess.upload_data(bucket=bucket, key_prefix=train_channel, path='./train_processed.txt')\n",
    "test_data_uri = sess.upload_data(bucket=bucket, key_prefix=test_channel, path='./test_processed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692583b3-f000-47eb-be05-f0f2245dc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 processed data files path\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, test_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e11f3c4-5444-4632-8578-dc9a2bdcf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model artifact output location\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddbc41-9fa3-4065-8673-30fc8b6bc903",
   "metadata": {},
   "source": [
    "## Training Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b7d9d-af56-4735-b8e8-ab6446da2e1b",
   "metadata": {},
   "source": [
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ***sagemaker.estimator.Estimator*** object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7b8940-b52a-4de8-bff1-e149db73d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:1 (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "# Blazingtext container \n",
    "image_uri = sagemaker.image_uris.retrieve(region=region, framework=\"blazingtext\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(image_uri, region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ec4ad9-92c0-4d7b-aa3b-b6f503c5f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blazingtext estimator instance passing the container image and hyperparameter setting\n",
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    image_uri,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 10,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"vector_dim\": 300,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 4,\n",
    "        \"min_epochs\": 5,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb641b6c-f1e9-4678-965c-2f8caed0318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data channels creation for algorithm\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70cd1705-8d62-4eef-a006-c333e4e9d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 21:20:48 Starting - Starting the training job...\n",
      "2022-09-20 21:21:16 Starting - Preparing the instances for trainingProfilerReport-1663708847: InProgress\n",
      ".........\n",
      "2022-09-20 21:22:37 Downloading - Downloading input data...\n",
      "2022-09-20 21:23:17 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 WARNING 140010953500480] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 WARNING 140010953500480] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 INFO 140010953500480] nvidia-smi took: 0.025332927703857422 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 INFO 140010953500480] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 INFO 140010953500480] Processing /opt/ml/input/data/train/train_processed.txt . File size: 1.9633750915527344 MB\u001b[0m\n",
      "\u001b[34m[09/20/2022 21:23:13 INFO 140010953500480] Processing /opt/ml/input/data/validation/test_processed.txt . File size: 1.965475082397461 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  15723\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/test_processed.txt\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0063  Progress: 36.82%  Million Words/sec: 3.39 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0051  Progress: 49.10%  Million Words/sec: 3.40 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0039  Progress: 61.36%  Million Words/sec: 3.41 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.313967\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0033  Progress: 67.48%  Million Words/sec: 0.88 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0020  Progress: 79.75%  Million Words/sec: 0.99 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.318801\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0008  Progress: 91.77%  Million Words/sec: 0.65 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.31952\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.00%  Million Words/sec: 0.49 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.319631\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 0.38 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 0.38\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 7.40\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.3307\u001b[0m\n",
      "\u001b[34mNumber of train examples: 54214\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.3196\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 54200\u001b[0m\n",
      "\n",
      "2022-09-20 21:23:40 Uploading - Uploading generated training model\n",
      "2022-09-20 21:28:26 Completed - Training job completed\n",
      "Training seconds: 363\n",
      "Billable seconds: 363\n"
     ]
    }
   ],
   "source": [
    "# Model fitting to the datasets \n",
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870d714-8059-4126-8978-edaf0597fa01",
   "metadata": {},
   "source": [
    "### Once the job has finished a “Job complete” message will be printed. The trained model can be found in the S3 bucket that was setup as output_path in the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0eb47-7d9e-4027-8a37-c1fcadf68df6",
   "metadata": {},
   "source": [
    "![S3 Model location](s3_model.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20810f43-50a9-48d4-a3b5-78a7ba32acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: No metrics called train:mean_rho found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:accuracy</td>\n",
       "      <td>0.3307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:accuracy</td>\n",
       "      <td>0.3196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp          metric_name   value\n",
       "0        0.0       train:accuracy  0.3307\n",
       "1        0.0  validation:accuracy  0.3196"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "bt_model.training_job_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d582ad-f051-458b-b5ef-add1b8e10c89",
   "metadata": {},
   "source": [
    "## Model Deploy & Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493b1e6-f609-4d16-ac9e-9b05d9fb3f2a",
   "metadata": {},
   "source": [
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28687721-049d-43f4-9bb0-07f25a89b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!\n",
      "Endpoint name:  blazingtext-2022-09-20-21-37-22-474\n"
     ]
    }
   ],
   "source": [
    "# Model Deployment as an endpoint \n",
    "text_classifier = bt_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m5.large',\n",
    "                                   serializer=sagemaker.serializers.JSONSerializer(),\n",
    "                                   deserializer=sagemaker.deserializers.JSONDeserializer())\n",
    "print()\n",
    "print('Endpoint name:  {}'.format(text_classifier.endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db8cd473-bdad-45a3-bbed-8622cae9b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': ['__label__documentary'], 'prob': [0.1242235079407692]}]\n"
     ]
    }
   ],
   "source": [
    "# Model Testing\n",
    "movie_name = ['Pink  Ribbons  One  Small  Step']\n",
    "\n",
    "payload = {\"instances\" : movie_name}\n",
    "predictions = text_classifier.predict(payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac081a34-fc94-4fe7-a832-29aa18ee9179",
   "metadata": {},
   "source": [
    "### Finally, we should delete the endpoint before we close the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f428f633-22e3-4dfe-bbc4-740cc74c1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up endpoint \n",
    "text_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe872dd1-4b40-4085-97c5-2cfd40b0999b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
